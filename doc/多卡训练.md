# 如何进行多卡训练
例子：yolov5多卡训练时参考：https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/


多卡训练方式有很多，主要有：DP、DDP 等

###DP

Pytorch的DP实现多GPU训练十分简单，只需在单GPU的基础上加一行代码即可。以下是一个DEMO的代码。
```
import torch
from torch import nn
from torch.optim import Adam
from torch.nn.parallel import DataParallel
class DEMO_model(nn.Module):
  def __init__(self, in_size, out_size):
    super().__init__()
    self.fc = nn.Linear(in_size, out_size)
  def forward(self, inp):
    outp = self.fc(inp)
    print(inp.shape, outp.device)
    return outp
model = DEMO_model(10, 5).to('cuda')
model = DataParallel(model, device_ids=[0, 1]) # 额外加这一行
adam = Adam(model.parameters())
# 进行训练
for i in range(1):
  x = torch.rand([128, 10]) # 获取训练数据，无需指定设备
  y = model(x) # 自动均匀划分数据批量并分配至各GPU，输出结果y会聚集到GPU0中
  loss = torch.norm(y)
  loss.backward()
  adam.step()

```

###DDP
为了对分布式编程有基本概念，首先使用pytorch内部的方法实现一个多进程程序，再使用DDP模块实现模型的分布式训练。

